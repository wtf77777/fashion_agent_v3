"""
å–®å¼µåœ–ç‰‡æ¨è«–è…³æœ¬
ç”¨æ–¼æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹
"""

import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
from pathlib import Path
from typing import Dict, List
import cv2

import config
from model import FashionMultiTaskModel


class FashionPredictor:
    """æœé£¾é æ¸¬å™¨"""
    
    def __init__(self, checkpoint_path: str = None):
        """
        Args:
            checkpoint_path: æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘ (None å‰‡ä½¿ç”¨ best.pth)
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è¼‰å…¥æ¨¡å‹
        self.model = FashionMultiTaskModel().to(self.device)
        
        if checkpoint_path is None:
            checkpoint_path = config.CHECKPOINT_DIR / 'best.pth'
        
        if Path(checkpoint_path).exists():
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… è¼‰å…¥æ¨¡å‹: {checkpoint_path}")
        else:
            print(f"âš ï¸  æ‰¾ä¸åˆ°æª¢æŸ¥é»: {checkpoint_path}")
            print("ä½¿ç”¨æœªè¨“ç·´çš„æ¨¡å‹")
        
        self.model.eval()
        
        # åœ–ç‰‡è½‰æ›
        self.transform = transforms.Compose([
            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    
    def predict(self, image_path: str, top_k: int = 3) -> Dict:
        """
        é æ¸¬å–®å¼µåœ–ç‰‡
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            top_k: è¿”å› Top-K é¡åˆ¥
        
        Returns:
            dict: é æ¸¬çµæœ
        """
        # è¼‰å…¥åœ–ç‰‡
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # è½‰æ›
        image_tensor = self.transform(image).unsqueeze(0).to(self.device)
        
        # é æ¸¬
        with torch.no_grad():
            pred = self.model.predict(image_tensor, threshold=config.ATTRIBUTE_THRESHOLD)
        
        # é¡åˆ¥é æ¸¬
        category_probs = pred['category_probs'][0].cpu().numpy()
        top_k_indices = np.argsort(category_probs)[-top_k:][::-1]
        
        top_k_categories = []
        for idx in top_k_indices:
            top_k_categories.append({
                'name': config.CATEGORY_NAMES[idx],
                'probability': float(category_probs[idx]),
                'index': int(idx)
            })
        
        # å±¬æ€§é æ¸¬
        attribute_probs = pred['attribute_probs'][0].cpu().numpy()
        attribute_pred = pred['attribute_pred'][0].cpu().numpy()
        
        active_attributes = []
        for i, is_active in enumerate(attribute_pred):
            if is_active:
                active_attributes.append({
                    'name': config.ATTRIBUTE_NAMES[i],
                    'probability': float(attribute_probs[i]),
                    'index': int(i)
                })
        
        # Embedding
        embedding = pred['embedding'][0].cpu().numpy()
        
        # æå–ä¸»è‰²èª¿
        dominant_colors = self.extract_dominant_colors(image_path)
        
        # æ¨æ–·é¢¨æ ¼æ¨™ç±¤
        style_tags = self.infer_style_tags(active_attributes)
        
        result = {
            'image_path': str(image_path),
            'image_size': original_size,
            'category': {
                'top_1': top_k_categories[0],
                'top_k': top_k_categories
            },
            'attributes': active_attributes,
            'colors': dominant_colors,
            'style_tags': style_tags,
            'embedding': embedding.tolist(),
            'embedding_dim': len(embedding)
        }
        
        return result
    
    def extract_dominant_colors(self, image_path: str, n_colors: int = 3) -> List[Dict]:
        """
        æå–ä¸»è‰²èª¿ (ä½¿ç”¨ K-Means)
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            n_colors: æå–é¡è‰²æ•¸é‡
        
        Returns:
            list: [{rgb, hex, percentage}, ...]
        """
        # è®€å–åœ–ç‰‡ (æ”¯æ´ä¸­æ–‡è·¯å¾‘)
        # cv2.imread ä¸æ”¯æ´ä¸­æ–‡è·¯å¾‘, æ”¹ç”¨ imdecode
        img_array = np.fromfile(str(image_path), dtype=np.uint8)
        image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
        
        if image is None:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")
            return []
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # èª¿æ•´å¤§å°ä»¥åŠ é€Ÿ
        image = cv2.resize(image, (150, 150))
        
        # é‡å¡‘ç‚ºåƒç´ åˆ—è¡¨ (float32)
        pixels = image.reshape(-1, 3).astype(np.float32)
        
        # ä½¿ç”¨ OpenCV çš„ K-Means
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        flags = cv2.KMEANS_RANDOM_CENTERS
        _, labels, centers = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)
        
        # ç²å–é¡è‰² (è½‰å› uint8)
        colors = centers.astype(int)
        labels = labels.flatten()
        
        # è¨ˆç®—æ¯å€‹é¡è‰²çš„æ¯”ä¾‹
        # labels æ˜¯ [0, 1, 2, 0, ...]
        counts = np.bincount(labels)
        percentages = counts / len(labels)
        
        # æŒ‰æ¯”ä¾‹æ’åº
        sorted_indices = np.argsort(percentages)[::-1]
        
        dominant_colors = []
        for idx in sorted_indices:
            rgb = colors[idx].tolist()
            hex_color = '#{:02x}{:02x}{:02x}'.format(*rgb)
            
            # ç°¡å–®çš„èƒŒæ™¯éæ¿¾: å¦‚æœé¡è‰²éæ–¼æ¥è¿‘ç´”ç™½ (sum > 700) æˆ–ç´”é»‘ (sum < 30) ä¸”ä½”æ¯” > 30%
            # è¦–ç‚ºèƒŒæ™¯å‰”é™¤ (é™¤éåªå‰©é€™å€‹é¡è‰²)
            color_sum = sum(rgb)
            if (color_sum > 700 or color_sum < 30) and percentages[idx] > 0.3:
                if len(sorted_indices) > 1 and len(dominant_colors) == 0:
                    continue  # è·³éèƒŒæ™¯è‰²
            
            dominant_colors.append({
                'rgb': rgb,
                'hex': hex_color,
                'percentage': float(percentages[idx])
            })
            
            if len(dominant_colors) >= n_colors:
                break
        
        # è¬ä¸€å…¨éƒ¨éƒ½è¢«éæ¿¾å…‰äº†(æ¥µç«¯æƒ…æ³)ï¼Œé€€å›åˆ°åŸå§‹çš„ç¬¬ä¸€å
        if not dominant_colors and len(sorted_indices) > 0:
            idx = sorted_indices[0]
            rgb = colors[idx].tolist()
            return [{
                'rgb': rgb,
                'hex': '#{:02x}{:02x}{:02x}'.format(*rgb),
                'percentage': float(percentages[idx])
            }]
        
        return dominant_colors
    
    def infer_style_tags(self, active_attributes: List[Dict]) -> List[str]:
        """
        æ ¹æ“šå±¬æ€§æ¨æ–·é¢¨æ ¼æ¨™ç±¤
        
        Args:
            active_attributes: å•Ÿç”¨çš„å±¬æ€§åˆ—è¡¨
        
        Returns:
            list: é¢¨æ ¼æ¨™ç±¤
        """
        attr_names = [attr['name'] for attr in active_attributes]
        
        style_tags = []
        for style, keywords in config.STYLE_MAPPING.items():
            if any(keyword in attr_names for keyword in keywords):
                style_tags.append(style)
        
        return style_tags
    
    def print_result(self, result: Dict):
        """æ‰“å°é æ¸¬çµæœ"""
        print("\n" + "="*60)
        print("ğŸ¯ é æ¸¬çµæœ")
        print("="*60)
        
        print(f"\nğŸ“¸ åœ–ç‰‡: {result['image_path']}")
        print(f"ğŸ“ å°ºå¯¸: {result['image_size']}")
        
        print(f"\nã€é¡åˆ¥é æ¸¬ã€‘")
        print(f"  ğŸ¥‡ Top-1: {result['category']['top_1']['name']} ({result['category']['top_1']['probability']*100:.2f}%)")
        
        print(f"\n  Top-{len(result['category']['top_k'])} é æ¸¬:")
        for i, cat in enumerate(result['category']['top_k'], 1):
            print(f"    {i}. {cat['name']:20s} {cat['probability']*100:.2f}%")
        
        print(f"\nã€å±¬æ€§é æ¸¬ã€‘ (å…± {len(result['attributes'])} å€‹)")
        for attr in result['attributes']:
            print(f"  âœ“ {attr['name']:20s} ({attr['probability']*100:.2f}%)")
        
        print(f"\nã€ä¸»è‰²èª¿ã€‘")
        for i, color in enumerate(result['colors'], 1):
            print(f"  {i}. RGB{tuple(color['rgb'])} {color['hex']} ({color['percentage']*100:.1f}%)")
        
        if result['style_tags']:
            print(f"\nã€é¢¨æ ¼æ¨™ç±¤ã€‘")
            for tag in result['style_tags']:
                print(f"  â€¢ {tag}")
        
        print(f"\nã€Embeddingã€‘")
        print(f"  ç¶­åº¦: {result['embedding_dim']}")
        print(f"  ç¯„ä¾‹å€¼: {result['embedding'][:5]}...")
        
        print("\n" + "="*60)


# ==================== ä¸»ç¨‹å¼ ====================
if __name__ == '__main__':
    import sys
    
    # å‰µå»ºé æ¸¬å™¨
    predictor = FashionPredictor()
    
    # æ¸¬è©¦åœ–ç‰‡è·¯å¾‘
    if len(sys.argv) > 1:
        image_path = sys.argv[1]
    else:
        # ä½¿ç”¨ç¯„ä¾‹åœ–ç‰‡ (ä¾†è‡ª train.txt çš„ç¬¬ä¸€å¼µåœ–)
        # train.txt è·¯å¾‘ç¯„ä¾‹: img/Sweet_Crochet_Blouse/img_00000070.jpg
        # config.IMG_DIR: .../model_a/.../data
        image_path = config.IMG_DIR / "img" / "Sweet_Crochet_Blouse" / "img_00000078.jpg"
        print(f"âš ï¸  æœªæŒ‡å®šåœ–ç‰‡è·¯å¾‘,ä½¿ç”¨ç¯„ä¾‹åœ–ç‰‡: {image_path}")
    
    if not Path(image_path).exists():
        print(f"âŒ åœ–ç‰‡ä¸å­˜åœ¨: {image_path}")
        sys.exit(1)
    
    # é æ¸¬
    result = predictor.predict(image_path, top_k=5)
    
    # æ‰“å°çµæœ
    predictor.print_result(result)
    
    # ä¿å­˜çµæœ
    import json
    output_path = config.RESULT_DIR / 'prediction_result.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜è‡³: {output_path}")
