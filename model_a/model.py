"""
Model A - å¤šä»»å‹™å­¸ç¿’æ¨¡å‹
åŒæ™‚é æ¸¬æœé£¾é¡åˆ¥å’Œå±¬æ€§
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Dict, Tuple
import config


class FashionMultiTaskModel(nn.Module):
    """æœé£¾å¤šä»»å‹™å­¸ç¿’æ¨¡å‹"""
    
    def __init__(
        self,
        num_categories: int = config.NUM_CATEGORIES,
        num_attributes: int = config.NUM_ATTRIBUTES,
        embedding_dim: int = config.EMBEDDING_DIM,
        backbone: str = config.BACKBONE,
        pretrained: bool = True
    ):
        """
        Args:
            num_categories: é¡åˆ¥æ•¸é‡ (50)
            num_attributes: å±¬æ€§æ•¸é‡ (26)
            embedding_dim: Embedding ç¶­åº¦ (512)
            backbone: é è¨“ç·´æ¨¡å‹åç¨±
            pretrained: æ˜¯å¦ä½¿ç”¨é è¨“ç·´æ¬Šé‡
        """
        super().__init__()
        
        self.num_categories = num_categories
        self.num_attributes = num_attributes
        self.embedding_dim = embedding_dim
        
        # ==================== Backbone ====================
        self.backbone, self.feature_dim = self._build_backbone(backbone, pretrained)
        
        # ==================== Embedding Layer ====================
        self.embedding_layer = nn.Sequential(
            nn.Linear(self.feature_dim, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3)
        )
        
        # ==================== Category Head ====================
        self.category_head = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_categories)
        )
        
        # ==================== Attribute Head ====================
        self.attribute_head = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_attributes)
        )
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        print(f"  - Backbone: {backbone}")
        print(f"  - Feature Dim: {self.feature_dim}")
        print(f"  - Embedding Dim: {embedding_dim}")
        print(f"  - Categories: {num_categories}")
        print(f"  - Attributes: {num_attributes}")
    
    def _build_backbone(self, backbone: str, pretrained: bool) -> Tuple[nn.Module, int]:
        """æ§‹å»º Backbone ç¶²è·¯"""
        
        if backbone == 'resnet50':
            model = models.resnet50(pretrained=pretrained)
            feature_dim = model.fc.in_features
            model.fc = nn.Identity()  # ç§»é™¤æœ€å¾Œçš„å…¨é€£æ¥å±¤
            
        elif backbone == 'efficientnet_b0':
            model = models.efficientnet_b0(pretrained=pretrained)
            feature_dim = model.classifier[1].in_features
            model.classifier = nn.Identity()
            
        elif backbone == 'mobilenet_v3_large':
            model = models.mobilenet_v3_large(pretrained=pretrained)
            feature_dim = model.classifier[0].in_features
            model.classifier = nn.Identity()
            
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ backbone: {backbone}")
        
        return model, feature_dim
    
    def forward(self, x: torch.Tensor, return_embedding: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥åœ–ç‰‡ [B, 3, H, W]
            return_embedding: æ˜¯å¦è¿”å› embedding
        
        Returns:
            dict: {
                'category_logits': [B, num_categories],
                'attribute_logits': [B, num_attributes],
                'embedding': [B, embedding_dim] (å¯é¸)
            }
        """
        # ç‰¹å¾µæå–
        features = self.backbone(x)  # [B, feature_dim]
        
        # Embedding
        embedding = self.embedding_layer(features)  # [B, embedding_dim]
        
        # é¡åˆ¥é æ¸¬
        category_logits = self.category_head(embedding)  # [B, num_categories]
        
        # å±¬æ€§é æ¸¬
        attribute_logits = self.attribute_head(embedding)  # [B, num_attributes]
        
        output = {
            'category_logits': category_logits,
            'attribute_logits': attribute_logits,
        }
        
        if return_embedding:
            output['embedding'] = embedding
        
        return output
    
    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> Dict[str, torch.Tensor]:
        """
        é æ¸¬æ¨¡å¼ (å¸¶ softmax/sigmoid)
        
        Args:
            x: è¼¸å…¥åœ–ç‰‡ [B, 3, H, W]
            threshold: å±¬æ€§é æ¸¬é–¾å€¼
        
        Returns:
            dict: {
                'category_probs': [B, num_categories],
                'category_pred': [B],
                'attribute_probs': [B, num_attributes],
                'attribute_pred': [B, num_attributes],
                'embedding': [B, embedding_dim]
            }
        """
        self.eval()
        with torch.no_grad():
            output = self.forward(x, return_embedding=True)
            
            # é¡åˆ¥é æ¸¬
            category_probs = torch.softmax(output['category_logits'], dim=1)
            category_pred = torch.argmax(category_probs, dim=1)
            
            # å±¬æ€§é æ¸¬
            attribute_probs = torch.sigmoid(output['attribute_logits'])
            attribute_pred = (attribute_probs > threshold).float()
            
            return {
                'category_probs': category_probs,
                'category_pred': category_pred,
                'attribute_probs': attribute_probs,
                'attribute_pred': attribute_pred,
                'embedding': output['embedding']
            }


class MultiTaskLoss(nn.Module):
    """å¤šä»»å‹™å­¸ç¿’æå¤±å‡½æ•¸"""
    
    def __init__(
        self,
        category_weight: float = 1.0,
        attribute_weight: float = 0.5,
        attribute_loss_type: str = 'bce'
    ):
        """
        Args:
            category_weight: é¡åˆ¥æå¤±æ¬Šé‡
            attribute_weight: å±¬æ€§æå¤±æ¬Šé‡
            attribute_loss_type: å±¬æ€§æå¤±é¡å‹ ('bce' æˆ– 'focal')
        """
        super().__init__()
        
        self.category_weight = category_weight
        self.attribute_weight = attribute_weight
        self.attribute_loss_type = attribute_loss_type
        
        # é¡åˆ¥æå¤± (Cross Entropy)
        self.category_loss_fn = nn.CrossEntropyLoss()
        
        # å±¬æ€§æå¤± (Binary Cross Entropy)
        if attribute_loss_type == 'bce':
            self.attribute_loss_fn = nn.BCEWithLogitsLoss()
        elif attribute_loss_type == 'focal':
            self.attribute_loss_fn = FocalLoss(alpha=config.FOCAL_ALPHA, gamma=config.FOCAL_GAMMA)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å±¬æ€§æå¤±é¡å‹: {attribute_loss_type}")
    
    def forward(self, outputs: Dict[str, torch.Tensor], targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è¨ˆç®—ç¸½æå¤±
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            targets: çœŸå¯¦æ¨™ç±¤
        
        Returns:
            dict: {
                'total_loss': ç¸½æå¤±,
                'category_loss': é¡åˆ¥æå¤±,
                'attribute_loss': å±¬æ€§æå¤±
            }
        """
        # é¡åˆ¥æå¤±
        category_loss = self.category_loss_fn(
            outputs['category_logits'],
            targets['category']
        )
        
        # å±¬æ€§æå¤±
        attribute_loss = self.attribute_loss_fn(
            outputs['attribute_logits'],
            targets['attributes']
        )
        
        # ç¸½æå¤±
        total_loss = (
            self.category_weight * category_loss +
            self.attribute_weight * attribute_loss
        )
        
        return {
            'total_loss': total_loss,
            'category_loss': category_loss,
            'attribute_loss': attribute_loss
        }


class FocalLoss(nn.Module):
    """Focal Loss for imbalanced classification"""
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss
        return F_loss.mean()


# ==================== æ¸¬è©¦ç¨‹å¼ç¢¼ ====================
if __name__ == '__main__':
    print("ğŸ§ª æ¸¬è©¦ Fashion Multi-Task Model")
    
    # å‰µå»ºæ¨¡å‹
    model = FashionMultiTaskModel()
    
    # æ¸¬è©¦è¼¸å…¥
    batch_size = 4
    x = torch.randn(batch_size, 3, config.IMG_SIZE, config.IMG_SIZE)
    
    print(f"\nğŸ“Š è¼¸å…¥å½¢ç‹€: {x.shape}")
    
    # å‰å‘å‚³æ’­
    output = model(x, return_embedding=True)
    
    print(f"\nğŸ“¦ è¼¸å‡ºå½¢ç‹€:")
    print(f"  - Category Logits: {output['category_logits'].shape}")
    print(f"  - Attribute Logits: {output['attribute_logits'].shape}")
    print(f"  - Embedding: {output['embedding'].shape}")
    
    # æ¸¬è©¦é æ¸¬
    pred = model.predict(x)
    
    print(f"\nğŸ¯ é æ¸¬çµæœ:")
    print(f"  - Category Probs: {pred['category_probs'].shape}")
    print(f"  - Category Pred: {pred['category_pred']}")
    print(f"  - Attribute Probs: {pred['attribute_probs'].shape}")
    print(f"  - Attribute Pred: {pred['attribute_pred'].shape}")
    
    # æ¸¬è©¦æå¤±å‡½æ•¸
    loss_fn = MultiTaskLoss()
    
    targets = {
        'category': torch.randint(0, config.NUM_CATEGORIES, (batch_size,)),
        'attributes': torch.randint(0, 2, (batch_size, config.NUM_ATTRIBUTES)).float()
    }
    
    losses = loss_fn(output, targets)
    
    print(f"\nğŸ’° æå¤±å€¼:")
    print(f"  - Total Loss: {losses['total_loss'].item():.4f}")
    print(f"  - Category Loss: {losses['category_loss'].item():.4f}")
    print(f"  - Attribute Loss: {losses['attribute_loss'].item():.4f}")
    
    # è¨ˆç®—åƒæ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ“ˆ æ¨¡å‹åƒæ•¸:")
    print(f"  - ç¸½åƒæ•¸é‡: {total_params:,}")
    print(f"  - å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
    
    print("\nâœ… æ¨¡å‹æ¸¬è©¦å®Œæˆ!")
